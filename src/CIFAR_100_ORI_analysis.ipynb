{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a86b75-71de-4e14-bbcf-f85f773b4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed first\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    recall_score, precision_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# SciPy and statistics\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import (\n",
    "    normaltest, skew, kurtosis, wasserstein_distance, \n",
    "    gaussian_kde, anderson\n",
    ")\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "\n",
    "# TensorFlow/Keras\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, MaxPooling3D, AveragePooling2D, \n",
    "    Dense, Flatten, BatchNormalization, Activation, \n",
    "    Input, Add, ZeroPadding2D, GlobalAveragePooling2D, \n",
    "    Dropout, LSTM, ConvLSTM2D, Reshape, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from scipy.stats import median_abs_deviation, lognorm\n",
    "\n",
    "sns.set_style(\"whitegrid\", {\"grid.color\": \".5\", \"grid.linestyle\": \":\"})\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08e7d4-3a9a-4a0b-91a7-b9c265538699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropout_rates = [0.8,0.6, 0.5, 0.4, 0.3, 0.2, 0.1] #0.5, 0.3, 0.2, 0.1\n",
    "new_ori_values = []\n",
    "    \n",
    "all_trends_dataframes = []\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING WITH DROPOUT RATE: {dropout_rate}\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    early_stopping_path = f'logs/early_stopping_point_dropout_{dropout_rate}.csv'\n",
    "    \n",
    "    data_metrics = pd.read_csv(f'logs/CIFAR100_ResNet50_dropout_{dropout_rate}.csv')\n",
    "    print(data_metrics.shape)\n",
    "    epochs=data_metrics.shape[0]\n",
    "    print('total epochs:',epochs)\n",
    "    loss_train=data_metrics['loss']  #training loss\n",
    "    loss_val=data_metrics['val_loss']  #validating loss\n",
    "    acc_train=data_metrics['accuracy'] #training accuracy\n",
    "    acc_val=data_metrics['val_accuracy'] #validating accuracy\n",
    "    \n",
    "    recall_train=data_metrics['recall']  #training recall\n",
    "    recall_val=data_metrics['val_recall']  #validating recall\n",
    "    precision_train=data_metrics['precision'] #training precision\n",
    "    precision_val=data_metrics['val_precision'] #validating precision\n",
    "    \n",
    "    AUC_train=data_metrics['AUC'] #training AUC\n",
    "    AUC_val=data_metrics['val_AUC'] #validating AUC\n",
    "    \n",
    "    gradients_training_loss1 = np.gradient(loss_train)  # 1st-order gradient of the training loss curve\n",
    "    gradients_validating_loss1 = np.gradient(loss_val)  # 1st-order gradient of the validating loss curve\n",
    "    \n",
    "    gradients_training_loss2 = np.gradient(gradients_training_loss1)   # 2nd-order gradient of the training loss curve\n",
    "    gradients_validating_loss2 = np.gradient(gradients_validating_loss1)  # 2nd-order gradient of the validating loss curve\n",
    "    \n",
    "    \n",
    "    loss_train=data_metrics['loss']  #training loss\n",
    "    loss_val=data_metrics['val_loss']  #validating loss\n",
    "    acc_train=data_metrics['accuracy'] #training accuracy\n",
    "    acc_val=data_metrics['val_accuracy'] #validating accuracy\n",
    "    \n",
    "    recall_train=data_metrics['recall']  #training recall\n",
    "    recall_val=data_metrics['val_recall']  #validating recall\n",
    "    precision_train=data_metrics['precision'] #training precision\n",
    "    precision_val=data_metrics['val_precision'] #validating precision\n",
    "    \n",
    "    AUC_train=data_metrics['AUC'] #training AUC\n",
    "    AUC_val=data_metrics['val_AUC'] #validating AUC\n",
    "    \n",
    "    gradients_training_loss1 = np.gradient(loss_train)  # 1st-order gradient of the training loss curve\n",
    "    gradients_validating_loss1 = np.gradient(loss_val)  # 1st-order gradient of the validating loss curve\n",
    "    \n",
    "    gradients_training_loss2 = np.gradient(gradients_training_loss1)   # 2nd-order gradient of the training loss curve\n",
    "    gradients_validating_loss2 = np.gradient(gradients_validating_loss1)  # 2nd-order gradient of the validating loss curve\n",
    "    \n",
    "    loss_training=loss_train.to_numpy()\n",
    "    loss_validating=loss_val.to_numpy()\n",
    "    acc_training=acc_train.to_numpy()\n",
    "    acc_validating=acc_val.to_numpy()\n",
    "    recall_training=recall_train.to_numpy()\n",
    "    recall_validating=recall_val.to_numpy()\n",
    "    precision_training=precision_train.to_numpy()\n",
    "    precision_validating=precision_val.to_numpy()\n",
    "    AUC_training=AUC_train.to_numpy()\n",
    "    AUC_validating=AUC_val.to_numpy()\n",
    "    \n",
    "\n",
    "    def wasserstein_flow_lognormal(actual_loss): \n",
    "        \n",
    "        log_loss=np.log(actual_loss + 1e-10)\n",
    "        \n",
    "        mu = np.median(log_loss)\n",
    "        sigma = median_abs_deviation(log_loss, scale='normal')\n",
    "                \n",
    "        n = len(actual_loss)\n",
    "        \n",
    "        sorted_actual = sorted(actual_loss)\n",
    "        q_ = np.linspace(0.5/n, 1-0.5/n, n)\n",
    "               \n",
    "        def objective_(params):\n",
    "            mu_opt, sigma_opt = params\n",
    "            ideal =  stats.lognorm.ppf(q_, s=sigma_opt, scale=np.exp(mu_opt))\n",
    "            return  np.sum((sorted_actual - ideal)**2) \n",
    "        \n",
    "        result_ = minimize(objective_, [mu, sigma], method='Nelder-Mead')\n",
    "        mu_opt_, sigma_opt_ = result_.x\n",
    "        print('best mu and sigma:',mu_opt_, sigma_opt_)\n",
    "        \n",
    "        return  stats.lognorm.ppf(q_, s=sigma_opt_, scale=np.exp(mu_opt_))\n",
    "        \n",
    "    fit_loss_training=wasserstein_flow_lognormal(loss_training) \n",
    "\n",
    "    mean_training = np.mean(loss_training)\n",
    "    mean_validation = np.mean(loss_validating)\n",
    "    median_training = np.median(loss_training)\n",
    "    median_validation = np.median(loss_validating)\n",
    "    \n",
    "    print(f\"Mean Training: {mean_training}, Mean Validation: {mean_validation}\")\n",
    "    print(f\"Median Training: {median_training}, Median Validation: {median_validation}\")\n",
    "\n",
    "    print('difference between validation and training:',mean_validation-mean_training,median_validation-median_training)\n",
    "   \n",
    "    fig = plt.figure(figsize=(40,30)) \n",
    "    grid = gridspec.GridSpec(9,9)  \n",
    "    \n",
    "    with open(early_stopping_path, mode='r') as file:  \n",
    "            reader = csv.reader(file)  \n",
    "            stop = int(next(reader)[0])  \n",
    "    intersection=0\n",
    "    start=1\n",
    "    end=len(loss_training)\n",
    "    epochs1=np.arange(start,len(loss_training)+start)\n",
    "    \n",
    "    ax1 = fig.add_subplot(grid[0,0])  \n",
    "    ax1.plot(loss_training,label='Training',linewidth=1,color='red')\n",
    "    ax1.plot(loss_validating,label='Validation',linewidth=1,color='green') #,linestyle='dashed'\n",
    "    ax1.plot(sorted(fit_loss_training,reverse=True),label='Ideal Training',linewidth=1,color='blue',linestyle='dashed') #,linestyle='dashed'\n",
    "    ax1.set_title('Loss Plot')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.legend(loc='best',fontsize=10) \n",
    "    \n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    d_it= wasserstein_l2(loss_training,fit_loss_training)\n",
    "    d_iv= wasserstein_l2(loss_validating,fit_loss_training)\n",
    "    d_vt= wasserstein_l2(loss_training,loss_validating)\n",
    "    \n",
    "    print('\\n\\033[1;31;45m Wasserstein distances:\\033[0m\\n ')\n",
    "    print('between training loss and validating loss:')\n",
    "    print(\"\\033[1;31;45m Wasserstein Distance :(new range) \\033[0m\", d_vt) \n",
    "    #wasserstein_dist(loss_training,loss_validating)\n",
    "    #wasserstein_dist_point(loss_training,loss_validating)\n",
    "    print('between training loss and ideal log normal')\n",
    "    print(\"\\033[1;31;40m Wasserstein Distance :(new range) \\033[0m\",d_it )\n",
    "    #wasserstein_dist(loss_training,fit_loss_training)\n",
    "    #wasserstein_dist_point(loss_training,fit_loss_training)\n",
    "    print('between ideal log normal and validating loss')\n",
    "    print(\"\\033[1;31;47m Wasserstein Distance :(new range) \\033[0m\", d_iv)\n",
    "    #wasserstein_dist(loss_validating,fit_loss_training)\n",
    "    #wasserstein_dist_point(loss_validating,fit_loss_training)\n",
    "    \n",
    "    \n",
    "    rank, assessment, explanation = evaluate_overfitting(d_it, d_iv, d_vt)\n",
    "    if mean_validation-mean_training<0 and median_validation-median_training<0:\n",
    "        new_ORI=0\n",
    "    else:\n",
    "        new_ORI=max(0,1-d_it/d_iv)\n",
    "    new_ori_values.append(new_ORI)\n",
    "    \n",
    "    print(f\"Rank Pattern: {rank}\")\n",
    "    print(f\"Overfitting Level: {assessment}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    \n",
    "    print('\\033[1;35;44m Wasserstein distance trend: train~validating\\033[0m',calculate_wasserstein_and_ori_trend(loss_training,loss_validating,fit_loss_training,start,end)[0])  #train~val\n",
    "    print('\\033[1;35;45m Wasserstein distance trend:ideal train~training\\033[0m',calculate_wasserstein_and_ori_trend(loss_training,loss_validating,fit_loss_training,start,end)[1])  # ideal train~val\n",
    "    print('\\033[1;35;46m Wasserstein distance trend: Ideal traing~validatning\\033[0m',calculate_wasserstein_and_ori_trend(loss_training,loss_validating,fit_loss_training,start,end)[2]) # train~ideal train\n",
    "    print('\\033[1;35;47m NEW ORI INDEX\\033[0m',new_ORI) # train~ideal train\n",
    "    print(new_ori_values)\n",
    "   \n",
    "    w1, w2, w3, ori_trend = calculate_wasserstein_and_ori_trend(loss_training,loss_validating,fit_loss_training,start,end)#Wasserstein_trend1(start, end)\n",
    "    \n",
    "    new_ORI = ori_trend[-1]\n",
    "    print(f\"Calculated ORI: {new_ORI:.4f}\")\n",
    "\n",
    "    trends_df = pd.DataFrame({\n",
    "            'epoch': range(1, len(loss_training) + 1),\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'ori_trend': ori_trend\n",
    "        })\n",
    "    all_trends_dataframes.append(trends_df)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 4.5))\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.plot(loss_training, label='Training', linewidth=1.5, color='red')\n",
    "    ax.plot(loss_validating, label='Validation', linewidth=1.5, color='green')\n",
    "    ax.plot(sorted(fit_loss_training,reverse=True), label='Ideal Training', linewidth=1.5, color='blue', linestyle='--')\n",
    "    ax.set_title('Loss Curves')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.axvline(stop, linestyle='--', color='skyblue', label=f'Early Stop (Epoch {stop})')\n",
    "    ax.annotate(' ES: epoch '+str(stop), xy=(stop-8, max(max(loss_training),max(loss_validating))*0.7),xytext=(stop-8, max(max(loss_training),max(loss_validating))*0.7),fontsize=10 )   #set text annotation for intersection line \n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "    ax = axs[1]\n",
    "    sns.kdeplot(loss_training, color='red', label='Training', ax=ax)\n",
    "    sns.kdeplot(loss_validating, color='green', label='Validation', ax=ax)\n",
    "    sns.kdeplot(fit_loss_training, color='blue', label='Ideal Training', linestyle='--', ax=ax)\n",
    "    ax.set_title('Density of Loss Values')\n",
    "    ax.set_xlabel('Loss Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "   \n",
    "    ax = axs[2]\n",
    "    ax.plot(w1, color='red', linewidth=2.0, linestyle='-', label='WD(Train, Valid)')\n",
    "    ax.plot(w2, color='black', linewidth=2.0, linestyle='--', label='WD(Train, Ideal)')\n",
    "    ax.plot(w3, color='green', linewidth=2.0, linestyle='-', label='WD(Valid, Ideal)')\n",
    "    ax.set_title('Wasserstein Distance Trends')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Distance')\n",
    "    #ax.axvline(stop, linestyle='--', color='#40BEE6')\n",
    "    ax.axvline(stop, linestyle='--', color='skyblue', label=f'Early Stop (Epoch {stop})')\n",
    "    ax.annotate(' ES: epoch '+str(stop), xy=(stop-8, max(max(w2),max(w3))*0.7),xytext=(stop-8,max(max(w2),max(w3))*0.7),fontsize=10 )   #set text annotation f\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "    \n",
    "\n",
    "    ax = axs[3]\n",
    "    ax.plot(ori_trend,color='#732BF5',linewidth =2.0,linestyle='solid',label='ORI Trend')\n",
    "    ax .set_title('ORI Trend')\n",
    "    ax .set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Values')\n",
    "    #ax9 .set_xlim(150,200)\n",
    "    ax.set_ylim(-0.01,1)\n",
    "    ax.vlines(stop, 0,1,linestyles='dashdot', colors='skyblue')  #plot earlystopping line\n",
    "    \n",
    "    ax.annotate(' ES: epoch '+str(stop), xy=(stop-8, max(ori_trend)*0.5),xytext=(stop-8,  max(ori_trend)*0.5),fontsize=10 )   #set text annotation for intersection line \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.legend(loc='best') \n",
    "\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f'logs/CIFAR100_ResNet50_dropout_{dropout_rate}.png', dpi=300)\n",
    "    plt.show()\n",
    "if all_trends_dataframes:\n",
    "    final_trends_df = pd.concat(all_trends_dataframes, ignore_index=True)\n",
    "    output_csv_path = 'results/all_per_epoch_ori_trends.csv'\n",
    "    final_trends_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSaved all per-epoch ori_trends to '{output_csv_path}'\")\n",
    "    plot_all_ori_trends(csv_path=output_csv_path)\n",
    "else:\n",
    "    print(\"\\nNo ori_trend data was collected. Skipping CSV saving and plotting.\")\n",
    "if len(new_ori_values) == len(dropout_rates):\n",
    "    ori_df = pd.DataFrame({\n",
    "        'dropout_rate': dropout_rates,\n",
    "        'new_ori': new_ori_values\n",
    "    })\n",
    "    \n",
    "    # Define the output path (e.g., in 'results' directory to match other outputs)\n",
    "    ori_output_csv_path = 'results/new_ori_values_by_dropout.csv'\n",
    "    \n",
    "    # Save to CSV\n",
    "    ori_df.to_csv(ori_output_csv_path, index=False)\n",
    "    print(f\"\\nSaved new_ori_values for all dropout rates to '{ori_output_csv_path}'\")\n",
    "else:\n",
    "    print(\"\\nWarning: Mismatch between dropout_rates and new_ori_values lengths. Skipping CSV save.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
