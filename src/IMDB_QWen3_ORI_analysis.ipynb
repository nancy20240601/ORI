{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a6c5c-fe00-4c76-a71f-6c2dc8acd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import lognorm, median_abs_deviation, wasserstein_distance, anderson, shapiro\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571da7c5-4431-4459-aaef-4a25b028e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wasserstein_flow_lognormal(actual_loss):\n",
    "   \n",
    "    log_loss = np.log(actual_loss + 1e-10)\n",
    "    \n",
    "  \n",
    "    mu = np.median(log_loss)\n",
    "    sigma = median_abs_deviation(log_loss, scale='normal')\n",
    "    \n",
    "    n = len(actual_loss)\n",
    "    sorted_actual = np.sort(actual_loss)\n",
    "    q_ = np.linspace(0.5 / n, 1 - 0.5 / n, n)\n",
    "    \n",
    "    def objective_(params):\n",
    "        mu_opt, sigma_opt = params\n",
    "        # Ensure sigma is positive to avoid math errors in lognorm.ppf\n",
    "        if sigma_opt <= 0:\n",
    "            return np.inf\n",
    "        ideal = stats.lognorm.ppf(q_, s=sigma_opt, scale=np.exp(mu_opt))\n",
    "        return np.sum((sorted_actual - ideal)**2)\n",
    "    \n",
    "    result_ = minimize(objective_, [mu, sigma], method='Nelder-Mead')\n",
    "    mu_opt_, sigma_opt_ = result_.x\n",
    "    print(f'Optimized Lognormal Params -> mu: {mu_opt_:.4f}, sigma: {sigma_opt_:.4f}')\n",
    "    \n",
    "    # Return the idealized loss values from the fitted distribution\n",
    "    return stats.lognorm.ppf(q_, s=sigma_opt_, scale=np.exp(mu_opt_))\n",
    "\n",
    "def wasserstein_l2(dist_a, dist_b):\n",
    "  \n",
    "  sorted_a = np.sort(dist_a)\n",
    "  sorted_b = np.sort(dist_b)\n",
    "  squared_diffs = (sorted_a - sorted_b)**2\n",
    "  return np.sqrt(np.mean(squared_diffs))\n",
    "\n",
    "def calculate_wasserstein_and_ori_trend(actual_training_loss, validation_loss, ideal_training_loss):\n",
    "    \n",
    "    start_epoch = 1\n",
    "    end_epoch = len(actual_training_loss)\n",
    "    d_train_valid_trend, d_ideal_train_trend, d_ideal_valid_trend, ori_trend = [], [], [], []\n",
    "\n",
    "    for i in range(start_epoch, end_epoch + 1):\n",
    "        train_window = actual_training_loss[:i]\n",
    "        valid_window = validation_loss[:i]\n",
    "        ideal_window = ideal_training_loss[:i]\n",
    "        \n",
    "        d_train_valid = wasserstein_l2(train_window, valid_window)\n",
    "        d_ideal_train = wasserstein_l2(ideal_window, train_window)\n",
    "        d_ideal_valid = wasserstein_l2(ideal_window, valid_window)\n",
    "        \n",
    "        d_train_valid_trend.append(d_train_valid)\n",
    "        d_ideal_train_trend.append(d_ideal_train)\n",
    "        d_ideal_valid_trend.append(d_ideal_valid)\n",
    "        \n",
    "        if np.mean(train_window) > np.mean(valid_window) and np.median(train_window) > np.median(valid_window):\n",
    "            ori = 0.0\n",
    "        else:\n",
    "            ori = 0.0 if d_ideal_valid == 0 else max(0.0, 1 - d_ideal_train / d_ideal_valid)\n",
    "        ori_trend.append(ori)\n",
    "\n",
    "    return d_train_valid_trend, d_ideal_train_trend, d_ideal_valid_trend, ori_trend\n",
    "\n",
    "def save_fit_loss_to_csv(fit_loss_training, actual_training_loss, output_dir):\n",
    "   \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.DataFrame({\n",
    "        \"epoch\": np.arange(1, len(fit_loss_training) + 1),\n",
    "        \"actual_training_loss\": np.asarray(actual_training_loss),\n",
    "        \"fit_loss_training\": np.asarray(fit_loss_training),\n",
    "        \"difference\": np.asarray(actual_training_loss) - np.asarray(fit_loss_training),\n",
    "    })\n",
    "    out_path = os.path.join(output_dir, \"fit_loss_training_data.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\" Saved fit_loss_training CSV to: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "try:\n",
    "    results_dirs = glob.glob(\"results*\")\n",
    "    if not results_dirs:\n",
    "        raise FileNotFoundError(\"No 'results directories found.\")\n",
    "    latest_dir = max(results_dirs, key=os.path.getmtime)\n",
    "    history_path = os.path.join(latest_dir, \"training_history.csv\")\n",
    "    if not os.path.exists(history_path):\n",
    "        raise FileNotFoundError(f\"training_history.csv not found in '{latest_dir}'\")\n",
    "    print(f\"Loading training history from: {history_path}\")\n",
    "    data_metrics = pd.read_csv(history_path)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "   \n",
    "    exit()\n",
    "\n",
    "\n",
    "loss_training = data_metrics['loss'].to_numpy()\n",
    "loss_validating = data_metrics['val_loss'].to_numpy()\n",
    "print(\"\\n--- Starting Overfitting Analysis ---\")\n",
    "fit_loss_training = wasserstein_flow_lognormal(loss_training)\n",
    "\n",
    "save_fit_loss_to_csv(fit_loss_training, loss_training, latest_dir)\n",
    "\n",
    "d_it = wasserstein_l2(loss_training, fit_loss_training)\n",
    "d_iv = wasserstein_l2(loss_validating, fit_loss_training)\n",
    "d_vt = wasserstein_l2(loss_training, loss_validating)\n",
    "print('\\n\\033[1;31;45m Wasserstein distances:\\033[0m\\n ')\n",
    "print('between training loss and validating loss:')\n",
    "print(\"\\033[1;31;45m Wasserstein Distance :(new range) \\033[0m\", d_vt) \n",
    "print('between training loss and ideal log normal')\n",
    "print(\"\\033[1;31;40m Wasserstein Distance :(new range) \\033[0m\",d_it )\n",
    "#wasserstein_dist(loss_training,fit_loss_training)\n",
    "#wasserstein_dist_point(loss_training,fit_loss_training)\n",
    "print('between ideal log normal and validating loss')\n",
    "print(\"\\033[1;31;47m Wasserstein Distance :(new range) \\033[0m\", d_iv)\n",
    "print('\\n--- Wasserstein Distances (Final) ---')\n",
    "print(f\"Training vs. Ideal (d_it): {d_it:.4f}\")\n",
    "print(f\"Validation vs. Ideal (d_iv): {d_iv:.4f}\")\n",
    "print(f\"Training vs. Validation (d_vt): {d_vt:.4f}\")\n",
    "'''\n",
    "final_ori = 0.0\n",
    "if not (np.mean(loss_training) > np.mean(loss_validating) and np.median(loss_training) > np.median(loss_validating)):\n",
    "    if d_iv > 0:\n",
    "        final_ori = max(0, 1 - d_it / d_iv)\n",
    "print(f\"Final ORI (Overfitting Resistance Index): {final_ori:.4f}\")\n",
    "'''\n",
    "w1, w2, w3, ori_trend = calculate_wasserstein_and_ori_trend(loss_training, loss_validating, fit_loss_training)\n",
    "\n",
    "print('ori trend:',ori_trend)\n",
    "trends_df = pd.DataFrame({  \n",
    "    'epoch': np.arange(1, len(ori_trend) + 1),  \n",
    "    \n",
    "    'ori_trend': ori_trend  \n",
    "})  \n",
    "\n",
    "# Define the output path and save the file  \n",
    "output_csv_path = os.path.join(latest_dir, 'overfitting_analysis_trends.csv')  \n",
    "trends_df.to_csv(output_csv_path, index=False)  \n",
    "# --- 3. PLOTTING ---\n",
    "print(\"\\n--- Generating Analysis Plot ---\")\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 4.5))\n",
    "#fig.suptitle(f'Analysis for DistilRoBERTa on IMDB Dataset', fontsize=16)\n",
    "\n",
    "# Plot 1: Loss Curves\n",
    "ax = axs[0]\n",
    "ax.plot(loss_training, label='Training', linewidth=1.5, color='red')\n",
    "ax.plot(loss_validating, label='Validation', linewidth=1.5, color='green')\n",
    "# FIX: Plotting fit_loss_training directly against epochs is correct.\n",
    "# Sorting it distorts the time-series representation of the ideal loss curve.\n",
    "ax.plot(sorted(fit_loss_training,reverse=True), label='Ideal Training', linewidth=1.5, color='blue', linestyle='--')\n",
    "ax.set_title('Loss Curves')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss Value')\n",
    "#ax.axvline(stop, linestyle='--', color='skyblue', label=f'Early Stop (Epoch {stop})')\n",
    "#ax.vlines(stop, 0,max(max(loss_training),max(loss_validating)),linestyles='dashdot', colors='skyblue')  #plot earlystopping line\n",
    "#ax.annotate(' ES: epoch '+str(stop), xy=(stop-8, max(max(loss_training),max(loss_validating))*0.7),xytext=(stop-8, max(max(loss_training),max(max(loss_validating))*0.7),fontsize=10 )   #set text annotation for intersection line \n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Density Plot\n",
    "ax = axs[1]\n",
    "sns.kdeplot(loss_training, color='red', label='Training', ax=ax)\n",
    "sns.kdeplot(loss_validating, color='green', label='Validation', ax=ax)\n",
    "sns.kdeplot(fit_loss_training, color='blue', label='Ideal Training', linestyle='--', ax=ax)\n",
    "ax.set_title('Density of Loss Values')\n",
    "ax.set_xlabel('Loss Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "# Plot 3: Wasserstein Distance Trends\n",
    "ax = axs[2]\n",
    "ax.plot(w1, color='red', linewidth=2.0, linestyle='-', label='WD(Train, Valid)')\n",
    "ax.plot(w2, color='black', linewidth=2.0, linestyle='--', label='WD(Train, Ideal)')\n",
    "ax.plot(w3, color='green', linewidth=2.0, linestyle='-', label='WD(Valid, Ideal)')\n",
    "ax.set_title('Wasserstein Distance Trends')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Distance')\n",
    "#ax.axvline(stop, linestyle='--', color='#40BEE6')\n",
    "#ax.axvline(stop, linestyle='--', color='skyblue', label=f'Early Stop (Epoch {stop})')\n",
    "#ax.vlines(stop, 0,max(max(loss_training),max(loss_validating)),linestyles='dashdot', colors='skyblue')  #plot earlystopping line\n",
    "#ax.annotate(' ES: epoch '+str(stop), xy=(stop-8, max(max(w2),max(w3))*0.7),xytext=(stop-8,max(max(w2),max(w3))*0.7),fontsize=10 )   #set text annotation f\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "\n",
    "# Plot 4: ORI Trend\n",
    "ax = axs[3]\n",
    "ax.plot(ori_trend, color='#732BF5', linewidth=2.0, linestyle='-')\n",
    "ax.set_title('Overfitting Robustness Index (ORI) Trend')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('ORI Value')\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "#ax.axvline(stop, linestyle='--', color='gray')\n",
    "ax.grid(True, which='both', linestyle=':', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "output_plot_path = os.path.join(latest_dir, 'DistilRoBERTa_imdb_overfitting_analysis.png')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(output_plot_path, dpi=250)\n",
    "print(f\"\\nAnalysis plot saved successfully to: {output_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "new_ORI = ori_trend[-1]\n",
    "print(f\"Calculated ORI: {new_ORI:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
